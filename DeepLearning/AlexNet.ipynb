{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "mnist_train = datasets.FashionMNIST(\"./dataset\", train=True, transform=transform)\n",
    "mnist_test = datasets.FashionMNIST(\"./dataset\", train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 96, 11, 2),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.Conv2d(96, 256, 5, padding=2),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.Conv2d(256, 384, 3, padding=1),\n",
    "    nn.Conv2d(384, 384, 3, padding=1),\n",
    "    nn.Conv2d(384, 256, 3, padding=1),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 12 * 12, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 96, 107, 107]          11,712\n",
      "         MaxPool2d-2           [-1, 96, 53, 53]               0\n",
      "            Conv2d-3          [-1, 256, 53, 53]         614,656\n",
      "         MaxPool2d-4          [-1, 256, 26, 26]               0\n",
      "            Conv2d-5          [-1, 384, 26, 26]         885,120\n",
      "            Conv2d-6          [-1, 384, 26, 26]       1,327,488\n",
      "            Conv2d-7          [-1, 256, 26, 26]         884,992\n",
      "         MaxPool2d-8          [-1, 256, 12, 12]               0\n",
      "           Flatten-9                [-1, 36864]               0\n",
      "           Linear-10                 [-1, 4096]     150,999,040\n",
      "             ReLU-11                 [-1, 4096]               0\n",
      "          Dropout-12                 [-1, 4096]               0\n",
      "           Linear-13                 [-1, 4096]      16,781,312\n",
      "             ReLU-14                 [-1, 4096]               0\n",
      "          Dropout-15                 [-1, 4096]               0\n",
      "           Linear-16                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 171,545,290\n",
      "Trainable params: 171,545,290\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 23.28\n",
      "Params size (MB): 654.39\n",
      "Estimated Total Size (MB): 677.87\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, (1, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d out size: torch.Size([1, 96, 107, 107])\n",
      "MaxPool2d out size: torch.Size([1, 96, 53, 53])\n",
      "Conv2d out size: torch.Size([1, 256, 53, 53])\n",
      "MaxPool2d out size: torch.Size([1, 256, 26, 26])\n",
      "Conv2d out size: torch.Size([1, 384, 26, 26])\n",
      "Conv2d out size: torch.Size([1, 384, 26, 26])\n",
      "Conv2d out size: torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d out size: torch.Size([1, 256, 12, 12])\n",
      "Flatten out size: torch.Size([1, 36864])\n",
      "Linear out size: torch.Size([1, 4096])\n",
      "ReLU out size: torch.Size([1, 4096])\n",
      "Dropout out size: torch.Size([1, 4096])\n",
      "Linear out size: torch.Size([1, 4096])\n",
      "ReLU out size: torch.Size([1, 4096])\n",
      "Dropout out size: torch.Size([1, 4096])\n",
      "Linear out size: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net.apply(init_weight)\n",
    "temp = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    temp = layer(temp)\n",
    "    print(layer.__class__.__name__, \"out size:\", temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, trainset, loss, optimizer, epochs):\n",
    "    try:\n",
    "        print(\"读取参数\")\n",
    "        net.load_state_dict(torch.load(\"./dataset/alexnet.pt\"))\n",
    "    except FileNotFoundError:\n",
    "        print(\"参数文件不存在,开始训练网络\")\n",
    "        net.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"第{epoch}次迭代,网络训练中...\")\n",
    "            for x, y in trainset:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                l = loss(net(x), y)\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "        torch.save(net.state_dict(), \"./dataset/alexnet.pt\")\n",
    "        print(\"网络训练完成,存储参数\")\n",
    "    else:\n",
    "        print(\"参数读取成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "net.to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainset = data.DataLoader(mnist_train, batch_size=16, shuffle=True)\n",
    "optimizer = torch.optim.SGD(net.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取参数\n",
      "参数文件不存在,开始训练网络\n",
      "第0次迭代,网络训练中...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkSpace\\PythonProject\\DeepLearning\\AlexNet.ipynb Cell 7'\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(net, trainset, loss, optimizer, epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m读取参数\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=3'>4</a>\u001b[0m     net\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m./dataset/alexnet.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=4'>5</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=696'>697</a>\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=698'>699</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=699'>700</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=700'>701</a>\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=701'>702</a>\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=702'>703</a>\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=229'>230</a>\u001b[0m \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=230'>231</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=231'>232</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=210'>211</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/envs/pytorch/lib/site-packages/torch/serialization.py?line=211'>212</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/alexnet.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkSpace\\PythonProject\\DeepLearning\\AlexNet.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000008?line=0'>1</a>\u001b[0m trainer(net, trainset, loss, optimizer, epochs)\n",
      "\u001b[1;32md:\\WorkSpace\\PythonProject\\DeepLearning\\AlexNet.ipynb Cell 7'\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(net, trainset, loss, optimizer, epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m第\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m次迭代,网络训练中...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m trainset:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=11'>12</a>\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkSpace/PythonProject/DeepLearning/AlexNet.ipynb#ch0000006?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer(net, trainset, loss, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "score_test = list()\n",
    "score_train = list()\n",
    "for a, b in mnist_test:\n",
    "    a = a.reshape(-1, 1, 224, 224)\n",
    "    score_test.append(net(a.cuda()).argmax() == b)\n",
    "for c, d in mnist_train:\n",
    "    if len(score_train) < 10000:\n",
    "        c = c.reshape(-1, 1, 224, 224)\n",
    "        score_train.append(net(c.cuda()).argmax() == d)\n",
    "    else:\n",
    "        break\n",
    "accuracy_test = sum(score_test) / len(score_test)\n",
    "accuracy_train = sum(score_train) / len(score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37b255bb5dc0d995b91bd1b934b878e610a26475f52eafaf29fdb395fb105534"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
