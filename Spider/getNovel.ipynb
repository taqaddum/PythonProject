{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["获取200个页面耗时1.1203410625457764秒\n","获取200个页面耗时1.3495452404022217秒\n","获取200个页面耗时1.1437115669250488秒\n","获取200个页面耗时1.3108525276184082秒\n","获取200个页面耗时1.5129530429840088秒\n","获取200个页面耗时1.1395964622497559秒\n","获取200个页面耗时1.320845603942871秒\n","获取200个页面耗时1.172393798828125秒\n","获取77个页面耗时1.0077250003814697秒\n"]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    # secs = random.random()\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        async with session.get(link, headers=header) as resp:\n","            return await resp.read()\n","    # await asyncio.sleep(secs)\n","    # return html\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","        time.sleep(0.5)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["获取200个页面耗时16.297772884368896秒\n","获取200个页面耗时4.675647497177124秒\n","获取200个页面耗时6.743532657623291秒\n","获取200个页面耗时6.023595809936523秒\n","获取200个页面耗时4.108102321624756秒\n","获取200个页面耗时8.21902871131897秒\n","获取200个页面耗时5.2087249755859375秒\n","获取200个页面耗时4.323275804519653秒\n","获取77个页面耗时5.453036785125732秒\n"]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    # secs = random.random()\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        async with session.get(link, headers=header) as resp:\n","            html =  await resp.read()\n","    # await asyncio.sleep(secs)\n","    return html\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","        time.sleep(0.5)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["获取200个页面耗时4.23507833480835秒\n","获取200个页面耗时3.1086573600769043秒\n","获取200个页面耗时3.1544721126556396秒\n","获取200个页面耗时3.7060840129852295秒\n","获取200个页面耗时4.870309352874756秒\n","获取200个页面耗时3.3080151081085205秒\n","获取200个页面耗时3.1477842330932617秒\n","获取200个页面耗时3.1457390785217285秒\n","获取77个页面耗时3.0622262954711914秒\n"]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    secs = random.randint(1, 2)\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        async with session.get(link, headers=header) as resp:\n","            await asyncio.sleep(secs)\n","            return await resp.read()\n","    # return html该位置返回值耗时长\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","        time.sleep(0.5)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["获取200个页面耗时3.034601926803589秒\n","获取200个页面耗时3.2510640621185303秒\n","获取200个页面耗时3.2358458042144775秒\n","获取200个页面耗时3.9722886085510254秒\n","获取200个页面耗时3.8287055492401123秒\n","获取200个页面耗时4.011605978012085秒\n","获取200个页面耗时3.7667717933654785秒\n","获取200个页面耗时2.132991313934326秒\n","获取77个页面耗时2.0240566730499268秒\n"]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        async with session.get(link, headers=header) as resp:\n","            await asyncio.sleep(1)\n","            return await resp.read()\n","    # return html该位置返回值耗时长\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["获取200个页面耗时2.2931997776031494秒\n","获取200个页面耗时2.2936036586761475秒\n","获取200个页面耗时2.3401360511779785秒\n","获取200个页面耗时2.190629005432129秒\n","获取200个页面耗时2.2158334255218506秒\n","获取200个页面耗时5.158896207809448秒\n","获取200个页面耗时5.726853132247925秒\n","获取200个页面耗时2.37021541595459秒\n","获取77个页面耗时2.2269649505615234秒\n"]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        await asyncio.sleep(1)\n","        async with session.get(link, headers=header) as resp:\n","            return await resp.read()\n","    # return html该位置返回值耗时长\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"TimeoutError","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32md:\\WorkSpace\\PythonProject\\Spider\\getNovel.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=81'>82</a>\u001b[0m href, title \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, getCatalogue(book))\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=82'>83</a>\u001b[0m htmls \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()  \u001b[39m# 保存网页\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=84'>85</a>\u001b[0m asyncio\u001b[39m.\u001b[39;49mrun(getBook(book, href, \u001b[39mlen\u001b[39;49m(href), \u001b[39m200\u001b[39;49m))\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=85'>86</a>\u001b[0m main(\u001b[39mzip\u001b[39m(htmls, title), \u001b[39m\"\u001b[39m\u001b[39m./download\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\nest_asyncio.py:38\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=35'>36</a>\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=36'>37</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=37'>38</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=38'>39</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=39'>40</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\nest_asyncio.py:81\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=77'>78</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=78'>79</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=79'>80</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=80'>81</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=198'>199</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=199'>200</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=200'>201</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=201'>202</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\tasks.py:258\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=255'>256</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=256'>257</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=257'>258</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49mthrow(exc)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=258'>259</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=259'>260</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_must_cancel:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=260'>261</a>\u001b[0m         \u001b[39m# Task is cancelled right before coro stops.\u001b[39;00m\n","\u001b[1;32md:\\WorkSpace\\PythonProject\\Spider\\getNovel.py\u001b[0m in \u001b[0;36mgetBook\u001b[1;34m(url, href, num, offset)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=62'>63</a>\u001b[0m began \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=63'>64</a>\u001b[0m tasks \u001b[39m=\u001b[39m (getChapter(url \u001b[39m+\u001b[39m \u001b[39mid\u001b[39m, book) \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m href[i : i \u001b[39m+\u001b[39m offset])\n\u001b[1;32m---> <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=64'>65</a>\u001b[0m chapters \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks)  \u001b[39m# note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=65'>66</a>\u001b[0m over \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=66'>67</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(num \u001b[39m-\u001b[39m i, offset)\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__wakeup\u001b[39m(\u001b[39mself\u001b[39m, future):\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=326'>327</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=327'>328</a>\u001b[0m         future\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=328'>329</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=329'>330</a>\u001b[0m         \u001b[39m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=330'>331</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__step(exc)\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=251'>252</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=252'>253</a>\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=253'>254</a>\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=254'>255</a>\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=255'>256</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=256'>257</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=257'>258</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n","\u001b[1;32md:\\WorkSpace\\PythonProject\\Spider\\getNovel.py\u001b[0m in \u001b[0;36mgetChapter\u001b[1;34m(link, referer)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=51'>52</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m ClientSession(connector\u001b[39m=\u001b[39mconnector, timeout\u001b[39m=\u001b[39mtimeout) \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=52'>53</a>\u001b[0m     \u001b[39m# await asyncio.sleep(1)\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=53'>54</a>\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m session\u001b[39m.\u001b[39mget(link, headers\u001b[39m=\u001b[39mheader) \u001b[39mas\u001b[39;00m resp:\n\u001b[1;32m---> <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=54'>55</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m resp\u001b[39m.\u001b[39mread()\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\client_reqrep.py:1036\u001b[0m, in \u001b[0;36mClientResponse.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1033'>1034</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_body \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1034'>1035</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1035'>1036</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_body \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mread()\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1036'>1037</a>\u001b[0m         \u001b[39mfor\u001b[39;00m trace \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traces:\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1037'>1038</a>\u001b[0m             \u001b[39mawait\u001b[39;00m trace\u001b[39m.\u001b[39msend_response_chunk_received(\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1038'>1039</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_body\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1039'>1040</a>\u001b[0m             )\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\streams.py:375\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=372'>373</a>\u001b[0m blocks \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=373'>374</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=374'>375</a>\u001b[0m     block \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadany()\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=375'>376</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=376'>377</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\streams.py:397\u001b[0m, in \u001b[0;36mStreamReader.readany\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=392'>393</a>\u001b[0m \u001b[39m# TODO: should be `if` instead of `while`\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=393'>394</a>\u001b[0m \u001b[39m# because waiter maybe triggered on chunk end,\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=394'>395</a>\u001b[0m \u001b[39m# without feeding any data\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=395'>396</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=396'>397</a>\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait(\u001b[39m\"\u001b[39m\u001b[39mreadany\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=398'>399</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_nowait(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\streams.py:304\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[1;34m(self, func_name)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=301'>302</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timer:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=302'>303</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timer:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=303'>304</a>\u001b[0m         \u001b[39mawait\u001b[39;00m waiter\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=304'>305</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=305'>306</a>\u001b[0m     \u001b[39mawait\u001b[39;00m waiter\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\helpers.py:721\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=717'>718</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks\u001b[39m.\u001b[39mpop()\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=719'>720</a>\u001b[0m \u001b[39mif\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m asyncio\u001b[39m.\u001b[39mCancelledError \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cancelled:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=720'>721</a>\u001b[0m     \u001b[39mraise\u001b[39;00m asyncio\u001b[39m.\u001b[39mTimeoutError \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=721'>722</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","\u001b[1;31mTimeoutError\u001b[0m: "]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        # await asyncio.sleep(1)\n","        async with session.get(link, headers=header) as resp:\n","            return await resp.read()\n","    # return html该位置返回值耗时最长\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["获取200个页面耗时1.9270589351654053秒\n","获取200个页面耗时3.471113681793213秒\n","获取200个页面耗时1.0841667652130127秒\n","获取200个页面耗时3.7214713096618652秒\n","获取200个页面耗时4.400755167007446秒\n"]},{"ename":"TimeoutError","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32md:\\WorkSpace\\PythonProject\\Spider\\getNovel.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=82'>83</a>\u001b[0m href, title \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, getCatalogue(book))\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=83'>84</a>\u001b[0m htmls \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()  \u001b[39m# 保存网页\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=85'>86</a>\u001b[0m asyncio\u001b[39m.\u001b[39;49mrun(getBook(book, href, \u001b[39mlen\u001b[39;49m(href), \u001b[39m200\u001b[39;49m))\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=86'>87</a>\u001b[0m main(\u001b[39mzip\u001b[39m(htmls, title), \u001b[39m\"\u001b[39m\u001b[39m./download\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\nest_asyncio.py:38\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=35'>36</a>\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=36'>37</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=37'>38</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=38'>39</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=39'>40</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\nest_asyncio.py:81\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=77'>78</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=78'>79</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=79'>80</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='file:///d%3A/miniconda3/lib/site-packages/nest_asyncio.py?line=80'>81</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=198'>199</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=199'>200</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=200'>201</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/futures.py?line=201'>202</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\tasks.py:258\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=255'>256</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=256'>257</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=257'>258</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49mthrow(exc)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=258'>259</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=259'>260</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_must_cancel:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=260'>261</a>\u001b[0m         \u001b[39m# Task is cancelled right before coro stops.\u001b[39;00m\n","\u001b[1;32md:\\WorkSpace\\PythonProject\\Spider\\getNovel.py\u001b[0m in \u001b[0;36mgetBook\u001b[1;34m(url, href, num, offset)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=62'>63</a>\u001b[0m began \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=63'>64</a>\u001b[0m tasks \u001b[39m=\u001b[39m (getChapter(url \u001b[39m+\u001b[39m \u001b[39mid\u001b[39m, book) \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m href[i : i \u001b[39m+\u001b[39m offset])\n\u001b[1;32m---> <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=64'>65</a>\u001b[0m chapters \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks)  \u001b[39m# note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=65'>66</a>\u001b[0m over \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=66'>67</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(num \u001b[39m-\u001b[39m i, offset)\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__wakeup\u001b[39m(\u001b[39mself\u001b[39m, future):\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=326'>327</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=327'>328</a>\u001b[0m         future\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=328'>329</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=329'>330</a>\u001b[0m         \u001b[39m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=330'>331</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__step(exc)\n","File \u001b[1;32mD:\\miniconda3\\lib\\asyncio\\tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=251'>252</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=252'>253</a>\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=253'>254</a>\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=254'>255</a>\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=255'>256</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=256'>257</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/asyncio/tasks.py?line=257'>258</a>\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n","\u001b[1;32md:\\WorkSpace\\PythonProject\\Spider\\getNovel.py\u001b[0m in \u001b[0;36mgetChapter\u001b[1;34m(link, referer)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=51'>52</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m ClientSession(connector\u001b[39m=\u001b[39mconnector, timeout\u001b[39m=\u001b[39mtimeout) \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=52'>53</a>\u001b[0m     \u001b[39m# await asyncio.sleep(1)\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=53'>54</a>\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m session\u001b[39m.\u001b[39mget(link, headers\u001b[39m=\u001b[39mheader) \u001b[39mas\u001b[39;00m resp:\n\u001b[1;32m---> <a href='file:///d%3A/WorkSpace/PythonProject/Spider/getNovel.py?line=54'>55</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m resp\u001b[39m.\u001b[39mread()\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\client_reqrep.py:1036\u001b[0m, in \u001b[0;36mClientResponse.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1033'>1034</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_body \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1034'>1035</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1035'>1036</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_body \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mread()\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1036'>1037</a>\u001b[0m         \u001b[39mfor\u001b[39;00m trace \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traces:\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1037'>1038</a>\u001b[0m             \u001b[39mawait\u001b[39;00m trace\u001b[39m.\u001b[39msend_response_chunk_received(\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1038'>1039</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_body\n\u001b[0;32m   <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/client_reqrep.py?line=1039'>1040</a>\u001b[0m             )\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\streams.py:375\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=372'>373</a>\u001b[0m blocks \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=373'>374</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=374'>375</a>\u001b[0m     block \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadany()\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=375'>376</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=376'>377</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\streams.py:397\u001b[0m, in \u001b[0;36mStreamReader.readany\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=392'>393</a>\u001b[0m \u001b[39m# TODO: should be `if` instead of `while`\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=393'>394</a>\u001b[0m \u001b[39m# because waiter maybe triggered on chunk end,\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=394'>395</a>\u001b[0m \u001b[39m# without feeding any data\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=395'>396</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=396'>397</a>\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait(\u001b[39m\"\u001b[39m\u001b[39mreadany\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=398'>399</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_nowait(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\streams.py:304\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[1;34m(self, func_name)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=301'>302</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timer:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=302'>303</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timer:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=303'>304</a>\u001b[0m         \u001b[39mawait\u001b[39;00m waiter\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=304'>305</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/streams.py?line=305'>306</a>\u001b[0m     \u001b[39mawait\u001b[39;00m waiter\n","File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\aiohttp\\helpers.py:721\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=717'>718</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks\u001b[39m.\u001b[39mpop()\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=719'>720</a>\u001b[0m \u001b[39mif\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m asyncio\u001b[39m.\u001b[39mCancelledError \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cancelled:\n\u001b[1;32m--> <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=720'>721</a>\u001b[0m     \u001b[39mraise\u001b[39;00m asyncio\u001b[39m.\u001b[39mTimeoutError \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/miniconda3/lib/site-packages/aiohttp/helpers.py?line=721'>722</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","\u001b[1;31mTimeoutError\u001b[0m: "]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        # await asyncio.sleep(1)\n","        async with session.get(link, headers=header) as resp:\n","            return await resp.read()\n","    # return html该位置返回值耗时最长\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","        time.sleep(0.5)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["获取200个页面耗时5.604754447937012秒\n","获取200个页面耗时4.615993499755859秒\n","获取200个页面耗时4.760902643203735秒\n","获取200个页面耗时5.117491722106934秒\n","获取200个页面耗时5.04739785194397秒\n","获取200个页面耗时2.1671507358551025秒\n","获取200个页面耗时2.7661960124969482秒\n","获取200个页面耗时4.3974151611328125秒\n","获取77个页面耗时2.032909870147705秒\n"]}],"source":["import asyncio\n","import random\n","import time\n","from turtle import title\n","\n","import aiohttp\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","from concurrent.futures import ProcessPoolExecutor\n","from pathlib import Path\n","\n","import requests\n","from aiohttp import ClientSession\n","from bs4 import BeautifulSoup\n","from faker import Faker\n","from lxml import etree\n","\n","\n","# 写入txt文本\n","def saveFile(html, name, dir):\n","    text = getContent(html)\n","    path = Path(dir).joinpath(name + \".txt\")\n","    path.write_text('\\n'.join((name, text)), encoding=\"gbk\", errors=\"ignore\")\n","\n","\n","# 解析html，提取小说内容\n","def getContent(html):\n","    root = BeautifulSoup(html, \"lxml\")\n","    text = root.find(\"div\", id=\"content\").get_text(\"\\n\")\n","    return text.replace(\"&nbsp;\", \"\")\n","\n","\n","# 同步，获取章节链接、章节名\n","def getCatalogue(url):\n","    header = {\"user-agent\": fake.chrome()}\n","    with requests.get(url, headers=header) as resp:\n","        resp.encoding = \"gbk\"\n","        tree = etree.HTML(resp.text)\n","        href = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/@href')\n","        title = tree.xpath('//*[@id=\"list\"]/dl/dd[position() >= 13]/a/text()')\n","    pags = list(zip(href, title))\n","    random.shuffle(pags)\n","    return zip(*pags)\n","\n","\n","# 异步，获取章节页面\n","async def getChapter(link, referer):\n","    connector = aiohttp.TCPConnector(limit=30)  # 限制并发数量，使用aiohttp参数而不是asyncio.SemaPhore\n","    timeout = aiohttp.ClientTimeout(total=20)  # 程序应在total秒内完成，否则报超时错误\n","    header = {\"user-agent\": fake.chrome(), \"referer\": referer}\n","    async with ClientSession(connector=connector, timeout=timeout) as session:\n","        await asyncio.sleep(1)\n","        async with session.get(link, headers=header) as resp:\n","            return await resp.read()\n","    # return html该位置返回值耗时最长\n","\n","\n","# 多阶段运行异步协程\n","async def getBook(url, href, num, offset):\n","    href = href[:num]\n","    for i in range(0, num, offset):\n","        began = time.time()\n","        tasks = (getChapter(url + id, book) for id in href[i : i + offset])\n","        chapters = await asyncio.gather(*tasks)  # note:不能塞入全部协程，否则服务器会拒绝访问，用循环划分\n","        over = time.time()\n","        count = min(num - i, offset)\n","        print(f\"获取{count}个页面耗时{over-began}秒\")\n","        htmls.extend(chapters)\n","        time.sleep(0.5)\n","\n","\n","# 多进程处理文本\n","def main(data, dir):  # note:进程池必须位于__main__ 主进程中，必须可以被工作者子进程导入，最好用函数封装\n","    with ProcessPoolExecutor() as future:\n","        for html, name in data:\n","            future.submit(saveFile, html=html, name=name, dir=dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fake = Faker()\n","    book = f\"https://www.xbiquge.so/book/4772/\"\n","    href, title = map(list, getCatalogue(book))\n","    htmls = list()  # 保存网页\n","\n","    asyncio.run(getBook(book, href, len(href), 200))\n","    main(zip(htmls, title), \"./download\")"]}],"metadata":{"interpreter":{"hash":"37b255bb5dc0d995b91bd1b934b878e610a26475f52eafaf29fdb395fb105534"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
